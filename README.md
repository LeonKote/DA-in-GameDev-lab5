# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Папушев Роман Олегович
- РИ220947
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

- В скрипте RollerAgent.cs на 36-й строке написано условие, что если расстояние до цели составляет менее, чем 1.42 единицы, что и является искомым “коэффициентом корреляции”, то агенту зачисляется вознаграждение

```cs

float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

if(distanceToTarget < 1.42f)
{
	SetReward(1.0f);
	EndEpisode();
}

```

- Влияние данного коэффициента на обучение модели заключается в том, что его увеличение ускоряет процесс обучения, но при этом увеличивает количество ошибок, а его уменьшение, наоборот, замедляет обучение, но повышает точность модели, снижая количество ошибок.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

**`trainer_type`**: Тип тренера. В данном случае, используется PPO для обучения агента.

**`hyperparameters`**:
- **`batch_size`**: Размер пакета данных, который используется при обучении.
- **`buffer_size`**: Размер буфера воспроизведения (памяти), используемого для хранения предыдущих состояний агента и опыта.
- **`learning_rate`**: Скорость обучения, определяет размер шага при обновлении весов агента.
- **`beta`**: Коэффициент для регуляризации (энтропийные потери) в PPO.
- **`epsilon`**: Параметр PPO, который определяет, насколько большим может быть отношение новой и старой вероятностей действий.
- **`lambd`**: Коэффициент сглаживания (lambda) для вычисления дисконтированной суммы награды.
- **`num_epoch`**: Количество эпох (полных проходов через данные) при обучении.
- **`learning_rate_schedule`**: График изменения скорости обучения. В данном случае, линейный график.

**`network_settings`**:
- **`normalize`**: Флаг, указывающий, следует ли нормализовать входные данные агента.
- **`hidden_units`**: Количество нейронов в скрытом слое нейронной сети.
- **`num_layers`**: Количество скрытых слоев в нейронной сети.

**`reward_signals`**:
- **`extrinsic`**: Внешняя награда, которая обычно представляет собой награды, предоставляемые средой.
  - **`gamma`**: Фактор дисконтирования для будущих наград.
  - **`strength`**: Сила влияния внешней награды на обучение агента.

**`max_steps`**: Максимальное количество шагов в среде, после которого обучение завершается.

**`time_horizon`**: Горизонт времени, который определяет, как долго в прошлое смотрит агент при сборе обучающих данных.

**`summary_freq`**: Частота создания сводных данных (например, для TensorBoard) в процессе обучения. В данном случае, каждые 10 000 шагов.


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

1. **Обучение персональных агентов в играх:**
   - Обучение персональных агентов для стратегических игр, таких как шахматы или го, где ML-Agents могут обучиться игре через многократное взаимодействие с средой.

2. **Управление персональными персонажами:**
   - Управление персональными персонажами в среде ролевой игры. ML-Agents могут обучить персонажей выполнять различные действия, такие как навигация по уровню, сражение и взаимодействие с объектами.

3. **Обучение групповых агентов:**
   - Обучение группы боевых отрядов в стратегической игре. ML-Agents могут научить агентов сотрудничать, разрабатывать тактику и адаптироваться к изменяющейся среде.

4. **Управление транспортными средствами:**
   - Обучение автомобильных агентов в гоночной игре для улучшенного вождения и принятия решений на трассе.

5. **Симуляция физики и управление роботами:**
   - Обучение роботов в симуляции для выполнения различных задач, таких как ходьба, поднятие предметов и решение задач, требующих управления физическими объектами.

В случаях, когда проще использовать ML-Agents, а не писать программную реализацию, важны следующие моменты:

- **Сложные поведенческие модели:** Если задача требует сложных и адаптивных стратегий, которые сложно или невозможно жестко программировать, ML-Agents могут быть более эффективными.

- **Автоматизированное обучение:** ML-Agents позволяют автоматизировать процесс обучения, что упрощает создание умных агентов без необходимости вручную настраивать большое количество параметров.

- **Адаптация к изменяющейся среде:** В средах, где условия изменяются или требуется адаптация к новым сценариям, ML-Agents могут быть более гибкими и способными к быстрой реакции на изменения.

## Выводы

В процессе работы я научился использовать программные средства для создания системы машинного обучения и ее интеграции в Unity. Я узнал, как использовать внутри C# скрипта “коэффициент корреляции”, как он влияет на обучение модели; как изменять параметры файла yaml-агента, какие параметры и как влияют на обучение модели, для каких игровых задачи и ситуаций могут использоваться ML-Agent’ы, и в каких случаях проще использовать ML-агент, а не писать программную реализацию решения.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
